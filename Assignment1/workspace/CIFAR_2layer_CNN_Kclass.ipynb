{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting Data from the file\n",
    "    Read the CIFAR data file. This has 5 train batches of size 10K each and one test batch of size 10k.\n",
    "    Each image is 32*32*3 == 3072\n",
    "    So each batch has dimension of 10,000*3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [{},{},{},{},{},{}] \n",
    "for i in range(1,6):\n",
    "    train_data[i] = unpickle(\"/Users/dibu/Gatech Academics/BigDataSystems-CS6220/Assignments/A1/workspace/cifar-10-batches-py/data_batch_\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=unpickle(\"/Users/dibu/Gatech Academics/BigDataSystems-CS6220/Assignments/A1/workspace/cifar-10-batches-py/test_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'batch_label'\n",
      "b'labels'\n",
      "b'data'\n",
      "b'filenames'\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "for x in train_data[1]:\n",
    "  print(x)\n",
    "print(train_data[i][b'data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i in range(1,6):\n",
    "        x.append(train_data[i][b'data'])\n",
    "        y.append(train_data[i][b'labels'])\n",
    "    \n",
    "    xtr = np.concatenate(x).reshape(50000,3,32,32).transpose(0,2,3,1)\n",
    "    ytr = np.concatenate(y)\n",
    "    xte = test_data[b'data'].reshape(10000,3,32,32).transpose(0,2,3,1)\n",
    "    yte = np.array(test_data[b'labels'])\n",
    "    \n",
    "    min_image =  np.min(xtr, axis=0)\n",
    "    max_image = np.max(xtr, axis = 0)\n",
    "    xtr = (xtr - min_image) / (max_image-min_image)\n",
    "    xte = (xte - min_image) / (max_image-min_image)\n",
    "    \n",
    "    \n",
    "    return xtr,ytr,xte,yte,np.arange(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_onehot(X, num_classes):\n",
    "    \n",
    "    num_classes = tf.constant(num_classes, name=\"num_classes\")\n",
    "    one_hot_matrix = tf.one_hot(X, depth= num_classes, axis= 1)\n",
    "    session = tf.Session()\n",
    "    X_one_hot = session.run(one_hot_matrix)\n",
    "    session.close()\n",
    "    return X_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test, classes = load_dataset()\n",
    "Y_train = convert_to_onehot(Y_train,10);\n",
    "Y_test = convert_to_onehot(Y_test,10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "Y_train shape: (50000, 10)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "Y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape: \"+ str(X_train.shape))\n",
    "print(\"Y_train shape: \"+ str(Y_train.shape))\n",
    "print(\"X_test shape: \"+ str(X_test.shape))\n",
    "print(\"Y_test shape: \"+ str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Sample Data   \n",
    "    (Train Data: Test Data) == (80:20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Sample shape: (10000, 32, 32, 3)\n",
      "Y_train Sample shape: (10000, 10)\n",
      "X_test Sample shape: (2000, 32, 32, 3)\n",
      "Y_test Sample shape: (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "sample = np.random.randint(low=0, high=50000, size=10000)\n",
    "X_train_sample = X_train[sample]\n",
    "Y_train_sample = Y_train[sample]\n",
    "\n",
    "\n",
    "sample = np.random.randint(low=0, high=10000, size=2000)\n",
    "X_test_sample = X_test[sample]\n",
    "Y_test_sample = Y_test[sample]\n",
    "\n",
    "print(\"X_train Sample shape: \"+ str(X_train_sample.shape))\n",
    "print(\"Y_train Sample shape: \"+ str(Y_train_sample.shape))\n",
    "print(\"X_test Sample shape: \"+ str(X_test_sample.shape))\n",
    "print(\"Y_test Sample shape: \"+ str(Y_test_sample.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 -  Tensorflow Forward Pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    X = tf.placeholder(tf.float32, shape=(None,n_H0,n_W0,n_C0))\n",
    "    Y = tf.placeholder(tf.float32, shape=(None,n_y))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Initialize Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \n",
    "    tf.set_random_seed(1)                              # so that your \"random\" numbers match ours\n",
    "        \n",
    "    W1 = tf.get_variable(\"W1\", [4,4,3,8], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W2 = tf.get_variable(\"W2\", [2,2,8,16], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    \n",
    "    #Conv2d\n",
    "    Z1 = tf.nn.conv2d(X,W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "#     print(Z1.shape)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "#     print(A1.shape)\n",
    "    # MAXPOOL\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n",
    "#     print(P1.shape)\n",
    "    # CONV2D\n",
    "    Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "#     print(Z2.shape)\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "#     print(A2.shape)\n",
    "    # MAXPOOL\n",
    "    P2 = tf.nn.max_pool(A2, ksize=[1,4,4,1], strides= [1,4,4,1], padding='SAME')\n",
    "    # FLATTEN\n",
    "#     print(P2.shape)\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "#     print(P2.shape)\n",
    "    # FULLY-CONNECTED \n",
    "    Z3 = tf.contrib.layers.fully_connected(P2, 10, activation_fn=None)\n",
    "#     print(Z3.shape)\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation]\n",
    "    shuffled_Y = Y[permutation]\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.01,\n",
    "          num_epochs = 100, minibatch_size = 200, print_cost = True):\n",
    "    \n",
    "    ops.reset_default_graph()                         \n",
    "    tf.set_random_seed(1)                             \n",
    "    seed = 3                                          \n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape    \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        \n",
    "    \n",
    "    \n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X,parameters)\n",
    "    cost = compute_cost(Z3,Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size)\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "            \n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "        \n",
    "        \n",
    "        \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 - Compute Run time and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_function(f, *args):\n",
    "    \"\"\"\n",
    "    Call a function f with args and return the time (in seconds) that it took to execute.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    tic = time.time()\n",
    "    _, _, parameters = f(*args)\n",
    "    toc = time.time()\n",
    "    return toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "(?, 32, 32, 8)\n",
      "(?, 32, 32, 8)\n",
      "(?, 8, 8, 8)\n",
      "(?, 8, 8, 16)\n",
      "(?, 8, 8, 16)\n",
      "(?, 2, 2, 16)\n",
      "WARNING:tensorflow:From //anaconda3/envs/cs6220/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "(?, 64)\n",
      "(?, 10)\n",
      "WARNING:tensorflow:From <ipython-input-14-d63c9aaddfe2>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Cost after epoch 0: 2.174098\n",
      "Cost after epoch 5: 1.583532\n",
      "Cost after epoch 10: 1.463865\n",
      "Cost after epoch 15: 1.401956\n",
      "Cost after epoch 20: 1.378458\n",
      "Cost after epoch 25: 1.360480\n",
      "Cost after epoch 30: 1.327599\n",
      "Cost after epoch 35: 1.310673\n",
      "Cost after epoch 40: 1.288057\n",
      "Cost after epoch 45: 1.265911\n",
      "Cost after epoch 50: 1.260422\n",
      "Cost after epoch 55: 1.281195\n",
      "Cost after epoch 60: 1.269916\n"
     ]
    }
   ],
   "source": [
    "execution_time = time_function(model, X_train_sample, Y_train_sample, X_test_sample, Y_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Execution Time: \" + str(execution_time) + \" sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Outlier Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
